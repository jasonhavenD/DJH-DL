{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonhaven/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 19s 2us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.2168 - acc: 0.9362\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0965 - acc: 0.9710\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 15s 248us/step - loss: 0.0673 - acc: 0.9790\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.0534 - acc: 0.9830\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0428 - acc: 0.9863\n",
      "10000/10000 [==============================] - 0s 35us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06519555800661328, 0.9807]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow.tools.api.generator.api.keras.datasets.mnist in tensorflow.tools.api.generator.api.keras.datasets:\n",
      "\n",
      "NAME\n",
      "    tensorflow.tools.api.generator.api.keras.datasets.mnist - Imports for Python API.\n",
      "\n",
      "DESCRIPTION\n",
      "    This file is MACHINE GENERATED! Do not edit.\n",
      "    Generated by: tensorflow/tools/api/generator/create_python_api.py script.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "\n",
      "\n",
      "FILE\n",
      "    /home/jasonhaven/anaconda3/lib/python3.6/site-packages/tensorflow/tools/api/generator/api/keras/datasets/mnist/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.datasets.mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'load_data']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.keras.datasets.mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_data in module tensorflow.python.keras._impl.keras.datasets.mnist:\n",
      "\n",
      "load_data(path='mnist.npz')\n",
      "    Loads the MNIST dataset.\n",
      "    \n",
      "    Arguments:\n",
      "        path: path where to cache the dataset locally\n",
      "            (relative to ~/.keras/datasets).\n",
      "    \n",
      "    Returns:\n",
      "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.datasets.mnist.load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Sequential in module tensorflow.python.keras._impl.keras.engine.sequential:\n",
      "\n",
      "class Sequential(tensorflow.python.keras._impl.keras.engine.training.Model)\n",
      " |  Linear stack of layers.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      layers: list of layers to add to the model.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Optionally, the first layer can receive an `input_shape` argument:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(500,)))\n",
      " |  # Afterwards, we do automatic shape inference:\n",
      " |  model.add(Dense(32))\n",
      " |  \n",
      " |  # This is identical to the following:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_dim=500))\n",
      " |  \n",
      " |  # And to the following:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, batch_input_shape=(None, 500)))\n",
      " |  \n",
      " |  # Note that you can also omit the `input_shape` argument:\n",
      " |  # In that case the model gets built the first time you call `fit` (or other\n",
      " |  # training and evaluation methods).\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32))\n",
      " |  model.add(Dense(32))\n",
      " |  model.compile(optimizer=optimizer, loss=loss)\n",
      " |  # This builds the model for the first time:\n",
      " |  model.fit(x, y, batch_size=32, epochs=10)\n",
      " |  \n",
      " |  # Note that when using this delayed-build pattern (no input shape specified),\n",
      " |  # the model doesn't have any weights until the first call\n",
      " |  # to a training/evaluation method (since it isn't yet built):\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32))\n",
      " |  model.add(Dense(32))\n",
      " |  model.weights  # returns []\n",
      " |  \n",
      " |  # Whereas if you specify the input shape, the model gets built continuously\n",
      " |  # as you are adding layers:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(500,)))\n",
      " |  model.add(Dense(32))\n",
      " |  model.weights  # returns list of length 4\n",
      " |  \n",
      " |  When using the delayed-build pattern (no input shape specified), you can\n",
      " |  choose to manually build your model by calling `build(batch_input_shape)`:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32))\n",
      " |  model.add(Dense(32))\n",
      " |  model.build((None, 500))\n",
      " |  model.weights  # returns list of length 4\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sequential\n",
      " |      tensorflow.python.keras._impl.keras.engine.training.Model\n",
      " |      tensorflow.python.keras._impl.keras.engine.network.Network\n",
      " |      tensorflow.python.keras._impl.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.layers.base.Layer\n",
      " |      tensorflow.python.training.checkpointable.CheckpointableBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, layers=None, name=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add(self, layer)\n",
      " |      Adds a layer instance on top of the layer stack.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          layer: layer instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: If `layer` is not a layer instance.\n",
      " |          ValueError: In case the `layer` argument does not\n",
      " |              know its input shape.\n",
      " |          ValueError: In case the `layer` argument has\n",
      " |              multiple output tensors, or is already connected\n",
      " |              somewhere else (forbidden in `Sequential` models).\n",
      " |  \n",
      " |  build(self, input_shape=None)\n",
      " |      Creates the variables of the layer.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  pop(self)\n",
      " |      Removes the last layer in the model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: if there are no layers in the model.\n",
      " |  \n",
      " |  predict_classes(self, x, batch_size=32, verbose=0)\n",
      " |      Generate class predictions for the input samples.\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          batch_size: integer.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A numpy array of class predictions.\n",
      " |  \n",
      " |  predict_proba(self, x, batch_size=32, verbose=0)\n",
      " |      Generates class probability predictions for the input samples.\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          batch_size: integer.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Numpy array of probability predictions.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Instantiates a Model from its config (output of `get_config()`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: Model config dictionary.\n",
      " |          custom_objects: Optional dictionary mapping names\n",
      " |              (strings) to custom classes or functions to be\n",
      " |              considered during deserialization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A model instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of improperly formatted config dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  layers\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras._impl.keras.engine.training.Model:\n",
      " |  \n",
      " |  compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          optimizer: String (name of optimizer) or optimizer instance.\n",
      " |              See [optimizers](/optimizers).\n",
      " |          loss: String (name of objective function) or objective function.\n",
      " |              See [losses](/losses).\n",
      " |              If the model has multiple outputs, you can use a different loss\n",
      " |              on each output by passing a dictionary or a list of losses.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the sum of all individual losses.\n",
      " |          metrics: List of metrics to be evaluated by the model\n",
      " |              during training and testing.\n",
      " |              Typically you will use `metrics=['accuracy']`.\n",
      " |              To specify different metrics for different outputs of a\n",
      " |              multi-output model, you could also pass a dictionary,\n",
      " |              such as `metrics={'output_a': 'accuracy'}`.\n",
      " |          loss_weights: Optional list or dictionary specifying scalar\n",
      " |              coefficients (Python floats) to weight the loss contributions\n",
      " |              of different model outputs.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the *weighted sum* of all individual losses,\n",
      " |              weighted by the `loss_weights` coefficients.\n",
      " |              If a list, it is expected to have a 1:1 mapping\n",
      " |              to the model's outputs. If a tensor, it is expected to map\n",
      " |              output names (strings) to scalar coefficients.\n",
      " |          sample_weight_mode: If you need to do timestep-wise\n",
      " |              sample weighting (2D weights), set this to `\"temporal\"`.\n",
      " |              `None` defaults to sample-wise weights (1D).\n",
      " |              If the model has multiple outputs, you can use a different\n",
      " |              `sample_weight_mode` on each output by passing a\n",
      " |              dictionary or a list of modes.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted\n",
      " |              by sample_weight or class_weight during training and testing.\n",
      " |          target_tensors: By default, Keras will create placeholders for the\n",
      " |              model's target, which will be fed with the target data during\n",
      " |              training. If instead you would like to use your own\n",
      " |              target tensors (in turn, Keras will not expect external\n",
      " |              Numpy data for these targets at training time), you\n",
      " |              can specify them via the `target_tensors` argument. It can be\n",
      " |              a single tensor (for a single-output model), a list of tensors,\n",
      " |              or a dict mapping output names to target tensors.\n",
      " |          **kwargs: These arguments are passed to `tf.Session.run`.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid arguments for\n",
      " |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n",
      " |      Returns the loss value & metrics values for the model in test mode.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Numpy array of test data (if the model has a single input),\n",
      " |              or list of Numpy arrays (if the model has multiple inputs).\n",
      " |              If input layers in the model are named, you can also pass a\n",
      " |              dictionary mapping input names to Numpy arrays.\n",
      " |              `x` can be `None` (default) if feeding from\n",
      " |              TensorFlow data tensors.\n",
      " |          y: Numpy array of target (label) data\n",
      " |              (if the model has a single output),\n",
      " |              or list of Numpy arrays (if the model has multiple outputs).\n",
      " |              If output layers in the model are named, you can also pass a\n",
      " |              dictionary mapping output names to Numpy arrays.\n",
      " |              `y` can be `None` (default) if feeding from\n",
      " |              TensorFlow data tensors.\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per evaluation step.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |          verbose: 0 or 1. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the test samples, used for weighting the loss function.\n",
      " |              You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      " |          steps: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring the evaluation round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: in case of invalid arguments.\n",
      " |  \n",
      " |  evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Evaluates the model on a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data\n",
      " |      as accepted by `test_on_batch`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          generator: Generator yielding tuples (inputs, targets)\n",
      " |              or (inputs, targets, sample_weights)\n",
      " |              or an instance of Sequence (keras.utils.Sequence)\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          max_queue_size: maximum size for the generator queue\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process-based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean.\n",
      " |              If `True`, use process-based threading.\n",
      " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
      " |              Note that because this implementation relies on multiprocessing,\n",
      " |              you should not pass non-picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: in case of invalid arguments.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case the generator yields\n",
      " |              data in an invalid format.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)\n",
      " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Numpy array of training data (if the model has a single input),\n",
      " |              or list of Numpy arrays (if the model has multiple inputs).\n",
      " |              If input layers in the model are named, you can also pass a\n",
      " |              dictionary mapping input names to Numpy arrays.\n",
      " |              `x` can be `None` (default) if feeding from\n",
      " |              TensorFlow data tensors.\n",
      " |          y: Numpy array of target (label) data\n",
      " |              (if the model has a single output),\n",
      " |              or list of Numpy arrays (if the model has multiple outputs).\n",
      " |              If output layers in the model are named, you can also pass a\n",
      " |              dictionary mapping output names to Numpy arrays.\n",
      " |              `y` can be `None` (default) if feeding from\n",
      " |              TensorFlow data tensors.\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See [callbacks](/callbacks).\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate\n",
      " |              the loss and any model metrics\n",
      " |              on this data at the end of each epoch.\n",
      " |              The validation data is selected from the last samples\n",
      " |              in the `x` and `y` data provided, before shuffling.\n",
      " |          validation_data: tuple `(x_val, y_val)` or tuple\n",
      " |              `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data.\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |          shuffle: Boolean (whether to shuffle the training data\n",
      " |              before each epoch) or str (for 'batch').\n",
      " |              'batch' is a special option for dealing with the\n",
      " |              limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring one epoch finished and starting the\n",
      " |              next epoch. When training with input tensors such as\n",
      " |              TensorFlow data tensors, the default `None` is equal to\n",
      " |              the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined.\n",
      " |          validation_steps: Only relevant if `steps_per_epoch`\n",
      " |              is specified. Total number of steps (batches of samples)\n",
      " |              to validate before stopping.\n",
      " |          **kwargs: Used for backwards compatibility.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If the model was never compiled.\n",
      " |          ValueError: In case of mismatch between the provided input data\n",
      " |              and what the model expects.\n",
      " |  \n",
      " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
      " |      \n",
      " |      The generator is run in parallel to the model, for efficiency.\n",
      " |      For instance, this allows you to do real-time data augmentation\n",
      " |      on images on CPU in parallel to training your model on GPU.\n",
      " |      \n",
      " |      The use of `keras.utils.Sequence` guarantees the ordering\n",
      " |      and guarantees the single use of every input per epoch when\n",
      " |      using `use_multiprocessing=True`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          generator: A generator or an instance of `Sequence`\n",
      " |            (`keras.utils.Sequence`)\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |              The output of the generator must be either\n",
      " |              - a tuple `(inputs, targets)`\n",
      " |              - a tuple `(inputs, targets, sample_weights)`.\n",
      " |              This tuple (a single output of the generator) makes a single batch.\n",
      " |              Therefore, all arrays in this tuple must have the same length (equal\n",
      " |              to the size of this batch). Different batches may have different\n",
      " |                sizes.\n",
      " |              For example, the last batch of the epoch is commonly smaller than\n",
      " |                the\n",
      " |              others, if the size of the dataset is not divisible by the batch\n",
      " |                size.\n",
      " |              The generator is expected to loop over its data\n",
      " |              indefinitely. An epoch finishes when `steps_per_epoch`\n",
      " |              batches have been seen by the model.\n",
      " |          steps_per_epoch: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before declaring one epoch\n",
      " |              finished and starting the next epoch. It should typically\n",
      " |              be equal to the number of samples of your dataset\n",
      " |              divided by the batch size.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          epochs: Integer, total number of iterations on the data.\n",
      " |          verbose: Verbosity mode, 0, 1, or 2.\n",
      " |          callbacks: List of callbacks to be called during training.\n",
      " |          validation_data: This can be either\n",
      " |              - a generator for the validation data\n",
      " |              - a tuple (inputs, targets)\n",
      " |              - a tuple (inputs, targets, sample_weights).\n",
      " |          validation_steps: Only relevant if `validation_data`\n",
      " |              is a generator. Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(validation_data)` as a number of steps.\n",
      " |          class_weight: Dictionary mapping class indices to a weight\n",
      " |              for the class.\n",
      " |          max_queue_size: Integer. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process-based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean.\n",
      " |              If `True`, use process-based threading.\n",
      " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
      " |              Note that because this implementation relies on multiprocessing,\n",
      " |              you should not pass non-picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |          shuffle: Boolean. Whether to shuffle the order of the batches at\n",
      " |              the beginning of each epoch. Only used with instances\n",
      " |              of `Sequence` (`keras.utils.Sequence`).\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          initial_epoch: Epoch at which to start training\n",
      " |              (useful for resuming a previous training run)\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `History` object.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |          def generate_arrays_from_file(path):\n",
      " |              while 1:\n",
      " |                  f = open(path)\n",
      " |                  for line in f:\n",
      " |                      # create numpy arrays of input data\n",
      " |                      # and labels, from each line in the file\n",
      " |                      x1, x2, y = process_line(line)\n",
      " |                      yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
      " |                  f.close()\n",
      " |      \n",
      " |          model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
      " |                              steps_per_epoch=10000, epochs=10)\n",
      " |      ```\n",
      " |      Raises:\n",
      " |          ValueError: In case the generator yields\n",
      " |              data in an invalid format.\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose=0, steps=None)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: The input data, as a Numpy array\n",
      " |              (or list of Numpy arrays if the model has multiple outputs).\n",
      " |          batch_size: Integer. If unspecified, it will default to 32.\n",
      " |          verbose: Verbosity mode, 0 or 1.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of mismatch between the provided\n",
      " |              input data and the model's expectations,\n",
      " |              or in case a stateful model receives a number of samples\n",
      " |              that is not a multiple of the batch size.\n",
      " |  \n",
      " |  predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Generates predictions for the input samples from a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data as accepted by\n",
      " |      `predict_on_batch`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          generator: Generator yielding batches of input samples\n",
      " |              or an instance of Sequence (keras.utils.Sequence)\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          max_queue_size: Maximum size for the generator queue.\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process-based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean.\n",
      " |              If `True`, use process-based threading.\n",
      " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
      " |              Note that because this implementation relies on multiprocessing,\n",
      " |              you should not pass non-picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case the generator yields\n",
      " |              data in an invalid format.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input samples, as a Numpy array.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |  \n",
      " |  test_on_batch(self, x, y, sample_weight=None)\n",
      " |      Test the model on a single batch of samples.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Numpy array of test data,\n",
      " |              or list of Numpy arrays if the model has multiple inputs.\n",
      " |              If all inputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping input names to Numpy arrays.\n",
      " |          y: Numpy array of target data,\n",
      " |              or list of Numpy arrays if the model has multiple outputs.\n",
      " |              If all outputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping output names to Numpy arrays.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape (samples, sequence_length),\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              sample_weight_mode=\"temporal\" in compile().\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid user-provided arguments.\n",
      " |  \n",
      " |  train_on_batch(self, x, y, sample_weight=None, class_weight=None)\n",
      " |      Runs a single gradient update on a single batch of data.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Numpy array of training data,\n",
      " |              or list of Numpy arrays if the model has multiple inputs.\n",
      " |              If all inputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping input names to Numpy arrays.\n",
      " |          y: Numpy array of target data,\n",
      " |              or list of Numpy arrays if the model has multiple outputs.\n",
      " |              If all outputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping output names to Numpy arrays.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape (samples, sequence_length),\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              sample_weight_mode=\"temporal\" in compile().\n",
      " |          class_weight: Optional dictionary mapping\n",
      " |              class indices (integers) to\n",
      " |              a weight (float) to apply to the model's loss for the samples\n",
      " |              from this class during training.\n",
      " |              This can be useful to tell the model to \"pay more attention\" to\n",
      " |              samples from an under-represented class.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar training loss\n",
      " |          (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: In case of invalid user-provided arguments.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras._impl.keras.engine.network.Network:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  add_loss(self, *args, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Note that `add_loss` is not supported when executing eagerly. Instead,\n",
      " |      variable regularizers may be added through `add_variable`. Activity\n",
      " |      regularization is not supported directly (but such losses may be returned\n",
      " |      from `Layer.call()`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors.\n",
      " |        inputs: If anything other than None is passed, it signals the losses\n",
      " |          are conditional on some of the layer's inputs,\n",
      " |          and thus they should only be run where these inputs are available.\n",
      " |          This is the case for activity regularization losses, for instance.\n",
      " |          If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  add_variable(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a new variable to the layer, or gets an existing one; returns it.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: variable name.\n",
      " |        shape: variable shape.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: initializer instance (callable).\n",
      " |        regularizer: regularizer instance (callable).\n",
      " |        trainable: whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean, stddev).\n",
      " |          Note, if the current variable scope is marked as non-trainable\n",
      " |          then this parameter is ignored and any added variables are also\n",
      " |          marked as non-trainable.\n",
      " |        constraint: constraint instance (callable).\n",
      " |        partitioner: (optional) partitioner instance (callable).  If\n",
      " |          provided, when the requested variable is created it will be split\n",
      " |          into multiple partitions according to `partitioner`.  In this case,\n",
      " |          an instance of `PartitionedVariable` is returned.  Available\n",
      " |          partitioners include `tf.fixed_size_partitioner` and\n",
      " |          `tf.variable_axis_size_partitioner`.  For more details, see the\n",
      " |          documentation of `tf.get_variable` and the  \"Variable Partitioners\n",
      " |          and Sharding\" section of the API guide.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable.  Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance.  If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |  \n",
      " |  call(self, inputs, training=None, mask=None)\n",
      " |      Calls the model on new inputs.\n",
      " |      \n",
      " |      In this case `call` just reapplies\n",
      " |      all ops in the graph to the new inputs\n",
      " |      (e.g. build a new computational graph from the provided inputs).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: A tensor or list of tensors.\n",
      " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
      " |            the `Network` in training mode or inference mode.\n",
      " |          mask: A mask or list of masks. A mask can be\n",
      " |              either a tensor or None (no mask).\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor if there is a single output, or\n",
      " |          a list of tensors if there are more than one outputs.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieves a layer based on either its name (unique) or index.\n",
      " |      \n",
      " |      If `name` and `index` are both provided, `index` will take precedence.\n",
      " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          name: String, name of layer.\n",
      " |          index: Integer, index of layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid layer name or index.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Retrieves the weights of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A flat list of Numpy arrays.\n",
      " |  \n",
      " |  load_weights(self, filepath, by_name=False)\n",
      " |      Loads all layer weights from a HDF5 save file.\n",
      " |      \n",
      " |      If `by_name` is False (default) weights are loaded\n",
      " |      based on the network's topology, meaning the architecture\n",
      " |      should be the same as when the weights were saved.\n",
      " |      Note that layers that don't have weights are not taken\n",
      " |      into account in the topological ordering, so adding or\n",
      " |      removing layers is fine as long as they don't have weights.\n",
      " |      \n",
      " |      If `by_name` is True, weights are loaded into layers\n",
      " |      only if they share the same name. This is useful\n",
      " |      for fine-tuning or transfer-learning models where\n",
      " |      some of the layers have changed.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String, path to the weights file to load.\n",
      " |          by_name: Boolean, whether to load weights by name\n",
      " |              or by topological order.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: If h5py is not available.\n",
      " |  \n",
      " |  reset_states(self)\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, include_optimizer=True)\n",
      " |      Saves the model to a single HDF5 file.\n",
      " |      \n",
      " |      The savefile includes:\n",
      " |          - The model architecture, allowing to re-instantiate the model.\n",
      " |          - The model weights.\n",
      " |          - The state of the optimizer, allowing to resume training\n",
      " |              exactly where you left off.\n",
      " |      \n",
      " |      This allows you to save the entirety of the state of a model\n",
      " |      in a single file.\n",
      " |      \n",
      " |      Saved models can be reinstantiated via `keras.models.load_model`.\n",
      " |      The model returned by `load_model`\n",
      " |      is a compiled model ready to be used (unless the saved model\n",
      " |      was never compiled in the first place).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String, path to the file to save the weights to.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          include_optimizer: If True, save optimizer's state together.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      from keras.models import load_model\n",
      " |      \n",
      " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
      " |      del model  # deletes the existing model\n",
      " |      \n",
      " |      # returns a compiled model\n",
      " |      # identical to the previous one\n",
      " |      model = load_model('my_model.h5')\n",
      " |      ```\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True)\n",
      " |      Dumps all layer weights to a HDF5 file.\n",
      " |      \n",
      " |      The weight file has:\n",
      " |          - `layer_names` (attribute), a list of strings\n",
      " |              (ordered names of model layers).\n",
      " |          - For every layer, a `group` named `layer.name`\n",
      " |              - For every such layer group, a group attribute `weight_names`,\n",
      " |                  a list of strings\n",
      " |                  (ordered names of weights tensor of the layer).\n",
      " |              - For every weight in the layer, a dataset\n",
      " |                  storing the weight value, named after the weight tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String, path to the file to save the weights to.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: If h5py is not available.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the model.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: A list of Numpy arrays with shapes and types matching\n",
      " |              the output of `model.get_weights()`.\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided,\n",
      " |              defaults to `[.33, .55, .67, 1.]`.\n",
      " |          print_fn: Print function to use. Defaults to `print`.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `json.dumps()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  to_yaml(self, **kwargs)\n",
      " |      Returns a yaml string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a yaml save file, use\n",
      " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
      " |      \n",
      " |      `custom_objects` should be a dictionary mapping\n",
      " |      the names of custom losses / layers / etc to the corresponding\n",
      " |      functions / classes.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `yaml.dump()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A YAML string.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: if yaml module is not found.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras._impl.keras.engine.network.Network:\n",
      " |  \n",
      " |  input_spec\n",
      " |      Gets the network's input specs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of `InputSpec` instances (one per input to the model)\n",
      " |              or a single instance if the model has only one input.\n",
      " |  \n",
      " |  losses\n",
      " |      Retrieves the network's losses.\n",
      " |      \n",
      " |      Will only include losses that are either\n",
      " |      unconditional, or conditional on inputs to this model\n",
      " |      (e.g. will not include losses that depend on tensors\n",
      " |      that aren't inputs to this model).\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of loss tensors.\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  state_updates\n",
      " |      Returns the `updates` from all layers that are stateful.\n",
      " |      \n",
      " |      This is useful for separating training updates and\n",
      " |      state updates, e.g. when we need to update a layer's internal state\n",
      " |      during prediction.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |      Retrieves the network's updates.\n",
      " |      \n",
      " |      Will only include updates that are either\n",
      " |      unconditional, or conditional on inputs to this model\n",
      " |      (e.g. will not include updates that were created by layers of this model\n",
      " |      outside of the model).\n",
      " |      \n",
      " |      Effectively, `network.updates` behaves like `layer.updates`.\n",
      " |      \n",
      " |      Concrete example:\n",
      " |      \n",
      " |      ```python\n",
      " |        bn = keras.layers.BatchNormalization()\n",
      " |        x1 = keras.layers.Input(shape=(10,))\n",
      " |        _ = bn(x1)  # This creates 2 updates.\n",
      " |      \n",
      " |        x2 = keras.layers.Input(shape=(10,))\n",
      " |        y2 = bn(x2)  # This creates 2 more updates.\n",
      " |      \n",
      " |        # The BN layer has now 4 updates.\n",
      " |        self.assertEqual(len(bn.updates), 4)\n",
      " |      \n",
      " |        # Let's create a model from x2 to y2.\n",
      " |        model = keras.models.Model(x2, y2)\n",
      " |      \n",
      " |        # The model does not list all updates from its underlying layers,\n",
      " |        # but only the updates that are relevant to it. Updates created by layers\n",
      " |        # outside of the model are discarded.\n",
      " |        self.assertEqual(len(model.updates), 2)\n",
      " |      \n",
      " |        # If you keep calling the model, you append to its updates, just like\n",
      " |        # what happens for a layer.\n",
      " |        x3 = keras.layers.Input(shape=(10,))\n",
      " |        y3 = model(x3)\n",
      " |        self.assertEqual(len(model.updates), 4)\n",
      " |      \n",
      " |        # But if you call the inner BN layer independently, you don't affect\n",
      " |        # the model's updates.\n",
      " |        x4 = keras.layers.Input(shape=(10,))\n",
      " |        _ = bn(x4)\n",
      " |        self.assertEqual(len(model.updates), 4)\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  uses_learning_phase\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras._impl.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, *args, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the shape of the input(s).\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          *args: Additional positional arguments to be passed to `call()`. Only\n",
      " |            allowed in subclassed Models with custom call() signatures. In other\n",
      " |            cases, `Layer` inputs must be passed using the `inputs` argument and\n",
      " |            non-inputs must be keyword arguments.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |          TypeError: If positional arguments are passed and this `Layer` is not a\n",
      " |              subclassed `Model`.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras._impl.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.layers.base.Layer:\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored in Eager mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops.\n",
      " |        inputs: If anything other than None is passed, it signals the updates\n",
      " |          are conditional on some of the layer's inputs,\n",
      " |          and thus they should only be run where these inputs are available.\n",
      " |          This is the case for BatchNormalization updates, for instance.\n",
      " |          If None, the updates will be taken into account unconditionally,\n",
      " |          and you are responsible for making sure that any dependency they might\n",
      " |          have is available at runtime.\n",
      " |          A step counter might fall into this category.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Apply the layer on a input.\n",
      " |      \n",
      " |      This simply wraps `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.layers.base.Layer:\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  graph\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  name\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  scope_name\n",
      " |  \n",
      " |  trainable_variables\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.checkpointable.CheckpointableBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nLinear stack of layers.\\n\\nArguments:\\n |      layers: list of layers to add to the model.\\n\\nadd(self, layer)\\n |      Adds a layer instance on top of the layer stack.\\nbuild(self, input_shape=None)\\n |      Creates the variables of the layer. \\n\\ncompile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\\n |      Configures the model for training.\\n |      \\n |      Arguments:\\n |          optimizer: String (name of optimizer) or optimizer instance.\\n |              See [optimizers](/optimizers).\\n |          loss: String (name of objective function) or objective function.\\n |              See [losses](/losses).\\n |              If the model has multiple outputs, you can use a different loss\\n |              on each output by passing a dictionary or a list of losses.\\n |              The loss value that will be minimized by the model\\n |              will then be the sum of all individual losses.\\n |          metrics: List of metrics to be evaluated by the model\\n |              during training and testing.\\n |              Typically you will use `metrics=[\\'accuracy\\']`.\\n |              To specify different metrics for different outputs of a\\n |              multi-output model, you could also pass a dictionary,\\n |              such as `metrics={\\'output_a\\': \\'accuracy\\'}`.\\n |          loss_weights: Optional list or dictionary specifying scalar\\n |              coefficients (Python floats) to weight the loss contributions\\n |              of different model outputs.\\n |              The loss value that will be minimized by the model\\n |              will then be the *weighted sum* of all individual losses,\\n |              weighted by the `loss_weights` coefficients.\\n |              If a list, it is expected to have a 1:1 mapping\\n |              to the model\\'s outputs. If a tensor, it is expected to map\\n |              output names (strings) to scalar coefficients.\\n |          sample_weight_mode: If you need to do timestep-wise\\n |              sample weighting (2D weights), set this to `\"temporal\"`.\\n |              `None` defaults to sample-wise weights (1D).\\n |              If the model has multiple outputs, you can use a different\\n |              `sample_weight_mode` on each output by passing a\\n |              dictionary or a list of modes.\\n |          weighted_metrics: List of metrics to be evaluated and weighted\\n |              by sample_weight or class_weight during training and testing.\\n |          target_tensors: By default, Keras will create placeholders for the\\n |              model\\'s target, which will be fed with the target data during\\n |              training. If instead you would like to use your own\\n |              target tensors (in turn, Keras will not expect external\\n |              Numpy data for these targets at training time), you\\n |              can specify them via the `target_tensors` argument. It can be\\n |              a single tensor (for a single-output model), a list of tensors,\\n |              or a dict mapping output names to target tensors.\\n |          **kwargs: These arguments are passed to `tf.Session.run`.\\n |      \\n |      Raises:\\n |          ValueError: In case of invalid arguments for\\n |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\\n |  \\n |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\\n |      Returns the loss value & metrics values for the model in test mode.\\n |      \\n |      Computation is done in batches.\\n |      \\n |      Arguments:\\n |          x: Numpy array of test data (if the model has a single input),\\n |              or list of Numpy arrays (if the model has multiple inputs).\\n |              If input layers in the model are named, you can also pass a\\n |              dictionary mapping input names to Numpy arrays.\\n |              `x` can be `None` (default) if feeding from\\n |              TensorFlow data tensors.\\n |          y: Numpy array of target (label) data\\n |              (if the model has a single output),\\n |              or list of Numpy arrays (if the model has multiple outputs).\\n |              If output layers in the model are named, you can also pass a\\n |              dictionary mapping output names to Numpy arrays.\\n |              `y` can be `None` (default) if feeding from\\n |              TensorFlow data tensors.\\n |          batch_size: Integer or `None`.\\n |              Number of samples per evaluation step.\\n |              If unspecified, `batch_size` will default to 32.\\n |          verbose: 0 or 1. Verbosity mode.\\n |              0 = silent, 1 = progress bar.\\n |          sample_weight: Optional Numpy array of weights for\\n |              the test samples, used for weighting the loss function.\\n |              You can either pass a flat (1D)\\n |              Numpy array with the same length as the input samples\\n |              (1:1 mapping between weights and samples),\\n |              or in the case of temporal data,\\n |              you can pass a 2D array with shape\\n |              `(samples, sequence_length)`,\\n |              to apply a different weight to every timestep of every sample.\\n |              In this case you should make sure to specify\\n |              `sample_weight_mode=\"temporal\"` in `compile()`.\\n |          steps: Integer or `None`.\\n |              Total number of steps (batches of samples)\\n |              before declaring the evaluation round finished.\\n |              Ignored with the default value of `None`.\\n |      \\n |      Returns:\\n |          Scalar test loss (if the model has a single output and no metrics)\\n |          or list of scalars (if the model has multiple outputs\\n |          and/or metrics). The attribute `model.metrics_names` will give you\\n |          the display labels for the scalar outputs.\\n |      \\n |      Raises:\\n |          ValueError: in case of invalid arguments.\\n |  \\n |  evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\\n |      Evaluates the model on a data generator.\\n |      \\n |      The generator should return the same kind of data\\n |      as accepted by `test_on_batch`.\\n |      \\n |      Arguments:\\n |          generator: Generator yielding tuples (inputs, targets)\\n |              or (inputs, targets, sample_weights)\\n |              or an instance of Sequence (keras.utils.Sequence)\\n |              object in order to avoid duplicate data\\n |              when using multiprocessing.\\n |          steps: Total number of steps (batches of samples)\\n |              to yield from `generator` before stopping.\\n |              Optional for `Sequence`: if unspecified, will use\\n |              the `len(generator)` as a number of steps.\\n |          max_queue_size: maximum size for the generator queue\\n |          workers: Integer. Maximum number of processes to spin up\\n |              when using process-based threading.\\n |              If unspecified, `workers` will default to 1. If 0, will\\n |              execute the generator on the main thread.\\n |          use_multiprocessing: Boolean.\\n |              If `True`, use process-based threading.\\n |              If unspecified, `use_multiprocessing` will default to `False`.\\n |              Note that because this implementation relies on multiprocessing,\\n |              you should not pass non-picklable arguments to the generator\\n |              as they can\\'t be passed easily to children processes.\\n |      \\n |      Returns:\\n |          Scalar test loss (if the model has a single output and no metrics)\\n |          or list of scalars (if the model has multiple outputs\\n |          and/or metrics). The attribute `model.metrics_names` will give you\\n |          the display labels for the scalar outputs.\\n |      \\n |      Raises:\\n |          ValueError: in case of invalid arguments.\\n |      \\n |      Raises:\\n |          ValueError: In case the generator yields\\n |              data in an invalid format.\\n |  \\n |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)\\n |      Trains the model for a fixed number of epochs (iterations on a dataset).\\n |      \\n |      Arguments:\\n |          x: Numpy array of training data (if the model has a single input),\\n |              or list of Numpy arrays (if the model has multiple inputs).\\n |              If input layers in the model are named, you can also pass a\\n |              dictionary mapping input names to Numpy arrays.\\n |              `x` can be `None` (default) if feeding from\\n |              TensorFlow data tensors.\\n |          y: Numpy array of target (label) data\\n |              (if the model has a single output),\\n |              or list of Numpy arrays (if the model has multiple outputs).\\n |              If output layers in the model are named, you can also pass a\\n |              dictionary mapping output names to Numpy arrays.\\n |              `y` can be `None` (default) if feeding from\\n |              TensorFlow data tensors.\\n |          batch_size: Integer or `None`.\\n |              Number of samples per gradient update.\\n |              If unspecified, `batch_size` will default to 32.\\n |          epochs: Integer. Number of epochs to train the model.\\n |              An epoch is an iteration over the entire `x` and `y`\\n |              data provided.\\n |              Note that in conjunction with `initial_epoch`,\\n |              `epochs` is to be understood as \"final epoch\".\\n |              The model is not trained for a number of iterations\\n |              given by `epochs`, but merely until the epoch\\n |              of index `epochs` is reached.\\n |          verbose: Integer. 0, 1, or 2. Verbosity mode.\\n |              0 = silent, 1 = progress bar, 2 = one line per epoch.\\n |          callbacks: List of `keras.callbacks.Callback` instances.\\n |              List of callbacks to apply during training.\\n |              See [callbacks](/callbacks).\\n |          validation_split: Float between 0 and 1.\\n |              Fraction of the training data to be used as validation data.\\n |              The model will set apart this fraction of the training data,\\n |              will not train on it, and will evaluate\\n |              the loss and any model metrics\\n |              on this data at the end of each epoch.\\n |              The validation data is selected from the last samples\\n |              in the `x` and `y` data provided, before shuffling.\\n |          validation_data: tuple `(x_val, y_val)` or tuple\\n |              `(x_val, y_val, val_sample_weights)` on which to evaluate\\n |              the loss and any model metrics at the end of each epoch.\\n |              The model will not be trained on this data.\\n |              `validation_data` will override `validation_split`.\\n |          shuffle: Boolean (whether to shuffle the training data\\n |              before each epoch) or str (for \\'batch\\').\\n |              \\'batch\\' is a special option for dealing with the\\n |              limitations of HDF5 data; it shuffles in batch-sized chunks.\\n |              Has no effect when `steps_per_epoch` is not `None`.\\n |          class_weight: Optional dictionary mapping class indices (integers)\\n |              to a weight (float) value, used for weighting the loss function\\n |              (during training only).\\n |              This can be useful to tell the model to\\n |              \"pay more attention\" to samples from\\n |              an under-represented class.\\n |          sample_weight: Optional Numpy array of weights for\\n |              the training samples, used for weighting the loss function\\n |              (during training only). You can either pass a flat (1D)\\n |              Numpy array with the same length as the input samples\\n |              (1:1 mapping between weights and samples),\\n |              or in the case of temporal data,\\n |              you can pass a 2D array with shape\\n |              `(samples, sequence_length)`,\\n |              to apply a different weight to every timestep of every sample.\\n |              In this case you should make sure to specify\\n |              `sample_weight_mode=\"temporal\"` in `compile()`.\\n |          initial_epoch: Integer.\\n |              Epoch at which to start training\\n |              (useful for resuming a previous training run).\\n |          steps_per_epoch: Integer or `None`.\\n |              Total number of steps (batches of samples)\\n |              before declaring one epoch finished and starting the\\n |              next epoch. When training with input tensors such as\\n |              TensorFlow data tensors, the default `None` is equal to\\n |              the number of samples in your dataset divided by\\n |              the batch size, or 1 if that cannot be determined.\\n |          validation_steps: Only relevant if `steps_per_epoch`\\n |              is specified. Total number of steps (batches of samples)\\n |              to validate before stopping.\\n |          **kwargs: Used for backwards compatibility.\\n |      \\n |      Returns:\\n |          A `History` object. Its `History.history` attribute is\\n |          a record of training loss values and metrics values\\n |          at successive epochs, as well as validation loss values\\n |          and validation metrics values (if applicable).\\n |      \\n |      Raises:\\n |          RuntimeError: If the model was never compiled.\\n |          ValueError: In case of mismatch between the provided input data\\n |              and what the model expects.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(tf.keras.models.Sequential)\n",
    "'''\n",
    "Linear stack of layers.\n",
    "\n",
    "Arguments:\n",
    " |      layers: list of layers to add to the model.\n",
    "\n",
    "add(self, layer)\n",
    " |      Adds a layer instance on top of the layer stack.\n",
    "build(self, input_shape=None)\n",
    " |      Creates the variables of the layer. \n",
    "\n",
    "compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\n",
    " |      Configures the model for training.\n",
    " |      \n",
    " |      Arguments:\n",
    " |          optimizer: String (name of optimizer) or optimizer instance.\n",
    " |              See [optimizers](/optimizers).\n",
    " |          loss: String (name of objective function) or objective function.\n",
    " |              See [losses](/losses).\n",
    " |              If the model has multiple outputs, you can use a different loss\n",
    " |              on each output by passing a dictionary or a list of losses.\n",
    " |              The loss value that will be minimized by the model\n",
    " |              will then be the sum of all individual losses.\n",
    " |          metrics: List of metrics to be evaluated by the model\n",
    " |              during training and testing.\n",
    " |              Typically you will use `metrics=['accuracy']`.\n",
    " |              To specify different metrics for different outputs of a\n",
    " |              multi-output model, you could also pass a dictionary,\n",
    " |              such as `metrics={'output_a': 'accuracy'}`.\n",
    " |          loss_weights: Optional list or dictionary specifying scalar\n",
    " |              coefficients (Python floats) to weight the loss contributions\n",
    " |              of different model outputs.\n",
    " |              The loss value that will be minimized by the model\n",
    " |              will then be the *weighted sum* of all individual losses,\n",
    " |              weighted by the `loss_weights` coefficients.\n",
    " |              If a list, it is expected to have a 1:1 mapping\n",
    " |              to the model's outputs. If a tensor, it is expected to map\n",
    " |              output names (strings) to scalar coefficients.\n",
    " |          sample_weight_mode: If you need to do timestep-wise\n",
    " |              sample weighting (2D weights), set this to `\"temporal\"`.\n",
    " |              `None` defaults to sample-wise weights (1D).\n",
    " |              If the model has multiple outputs, you can use a different\n",
    " |              `sample_weight_mode` on each output by passing a\n",
    " |              dictionary or a list of modes.\n",
    " |          weighted_metrics: List of metrics to be evaluated and weighted\n",
    " |              by sample_weight or class_weight during training and testing.\n",
    " |          target_tensors: By default, Keras will create placeholders for the\n",
    " |              model's target, which will be fed with the target data during\n",
    " |              training. If instead you would like to use your own\n",
    " |              target tensors (in turn, Keras will not expect external\n",
    " |              Numpy data for these targets at training time), you\n",
    " |              can specify them via the `target_tensors` argument. It can be\n",
    " |              a single tensor (for a single-output model), a list of tensors,\n",
    " |              or a dict mapping output names to target tensors.\n",
    " |          **kwargs: These arguments are passed to `tf.Session.run`.\n",
    " |      \n",
    " |      Raises:\n",
    " |          ValueError: In case of invalid arguments for\n",
    " |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
    " |  \n",
    " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n",
    " |      Returns the loss value & metrics values for the model in test mode.\n",
    " |      \n",
    " |      Computation is done in batches.\n",
    " |      \n",
    " |      Arguments:\n",
    " |          x: Numpy array of test data (if the model has a single input),\n",
    " |              or list of Numpy arrays (if the model has multiple inputs).\n",
    " |              If input layers in the model are named, you can also pass a\n",
    " |              dictionary mapping input names to Numpy arrays.\n",
    " |              `x` can be `None` (default) if feeding from\n",
    " |              TensorFlow data tensors.\n",
    " |          y: Numpy array of target (label) data\n",
    " |              (if the model has a single output),\n",
    " |              or list of Numpy arrays (if the model has multiple outputs).\n",
    " |              If output layers in the model are named, you can also pass a\n",
    " |              dictionary mapping output names to Numpy arrays.\n",
    " |              `y` can be `None` (default) if feeding from\n",
    " |              TensorFlow data tensors.\n",
    " |          batch_size: Integer or `None`.\n",
    " |              Number of samples per evaluation step.\n",
    " |              If unspecified, `batch_size` will default to 32.\n",
    " |          verbose: 0 or 1. Verbosity mode.\n",
    " |              0 = silent, 1 = progress bar.\n",
    " |          sample_weight: Optional Numpy array of weights for\n",
    " |              the test samples, used for weighting the loss function.\n",
    " |              You can either pass a flat (1D)\n",
    " |              Numpy array with the same length as the input samples\n",
    " |              (1:1 mapping between weights and samples),\n",
    " |              or in the case of temporal data,\n",
    " |              you can pass a 2D array with shape\n",
    " |              `(samples, sequence_length)`,\n",
    " |              to apply a different weight to every timestep of every sample.\n",
    " |              In this case you should make sure to specify\n",
    " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
    " |          steps: Integer or `None`.\n",
    " |              Total number of steps (batches of samples)\n",
    " |              before declaring the evaluation round finished.\n",
    " |              Ignored with the default value of `None`.\n",
    " |      \n",
    " |      Returns:\n",
    " |          Scalar test loss (if the model has a single output and no metrics)\n",
    " |          or list of scalars (if the model has multiple outputs\n",
    " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
    " |          the display labels for the scalar outputs.\n",
    " |      \n",
    " |      Raises:\n",
    " |          ValueError: in case of invalid arguments.\n",
    " |  \n",
    " |  evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    " |      Evaluates the model on a data generator.\n",
    " |      \n",
    " |      The generator should return the same kind of data\n",
    " |      as accepted by `test_on_batch`.\n",
    " |      \n",
    " |      Arguments:\n",
    " |          generator: Generator yielding tuples (inputs, targets)\n",
    " |              or (inputs, targets, sample_weights)\n",
    " |              or an instance of Sequence (keras.utils.Sequence)\n",
    " |              object in order to avoid duplicate data\n",
    " |              when using multiprocessing.\n",
    " |          steps: Total number of steps (batches of samples)\n",
    " |              to yield from `generator` before stopping.\n",
    " |              Optional for `Sequence`: if unspecified, will use\n",
    " |              the `len(generator)` as a number of steps.\n",
    " |          max_queue_size: maximum size for the generator queue\n",
    " |          workers: Integer. Maximum number of processes to spin up\n",
    " |              when using process-based threading.\n",
    " |              If unspecified, `workers` will default to 1. If 0, will\n",
    " |              execute the generator on the main thread.\n",
    " |          use_multiprocessing: Boolean.\n",
    " |              If `True`, use process-based threading.\n",
    " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
    " |              Note that because this implementation relies on multiprocessing,\n",
    " |              you should not pass non-picklable arguments to the generator\n",
    " |              as they can't be passed easily to children processes.\n",
    " |      \n",
    " |      Returns:\n",
    " |          Scalar test loss (if the model has a single output and no metrics)\n",
    " |          or list of scalars (if the model has multiple outputs\n",
    " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
    " |          the display labels for the scalar outputs.\n",
    " |      \n",
    " |      Raises:\n",
    " |          ValueError: in case of invalid arguments.\n",
    " |      \n",
    " |      Raises:\n",
    " |          ValueError: In case the generator yields\n",
    " |              data in an invalid format.\n",
    " |  \n",
    " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)\n",
    " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    " |      \n",
    " |      Arguments:\n",
    " |          x: Numpy array of training data (if the model has a single input),\n",
    " |              or list of Numpy arrays (if the model has multiple inputs).\n",
    " |              If input layers in the model are named, you can also pass a\n",
    " |              dictionary mapping input names to Numpy arrays.\n",
    " |              `x` can be `None` (default) if feeding from\n",
    " |              TensorFlow data tensors.\n",
    " |          y: Numpy array of target (label) data\n",
    " |              (if the model has a single output),\n",
    " |              or list of Numpy arrays (if the model has multiple outputs).\n",
    " |              If output layers in the model are named, you can also pass a\n",
    " |              dictionary mapping output names to Numpy arrays.\n",
    " |              `y` can be `None` (default) if feeding from\n",
    " |              TensorFlow data tensors.\n",
    " |          batch_size: Integer or `None`.\n",
    " |              Number of samples per gradient update.\n",
    " |              If unspecified, `batch_size` will default to 32.\n",
    " |          epochs: Integer. Number of epochs to train the model.\n",
    " |              An epoch is an iteration over the entire `x` and `y`\n",
    " |              data provided.\n",
    " |              Note that in conjunction with `initial_epoch`,\n",
    " |              `epochs` is to be understood as \"final epoch\".\n",
    " |              The model is not trained for a number of iterations\n",
    " |              given by `epochs`, but merely until the epoch\n",
    " |              of index `epochs` is reached.\n",
    " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
    " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
    " |              List of callbacks to apply during training.\n",
    " |              See [callbacks](/callbacks).\n",
    " |          validation_split: Float between 0 and 1.\n",
    " |              Fraction of the training data to be used as validation data.\n",
    " |              The model will set apart this fraction of the training data,\n",
    " |              will not train on it, and will evaluate\n",
    " |              the loss and any model metrics\n",
    " |              on this data at the end of each epoch.\n",
    " |              The validation data is selected from the last samples\n",
    " |              in the `x` and `y` data provided, before shuffling.\n",
    " |          validation_data: tuple `(x_val, y_val)` or tuple\n",
    " |              `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
    " |              the loss and any model metrics at the end of each epoch.\n",
    " |              The model will not be trained on this data.\n",
    " |              `validation_data` will override `validation_split`.\n",
    " |          shuffle: Boolean (whether to shuffle the training data\n",
    " |              before each epoch) or str (for 'batch').\n",
    " |              'batch' is a special option for dealing with the\n",
    " |              limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
    " |              Has no effect when `steps_per_epoch` is not `None`.\n",
    " |          class_weight: Optional dictionary mapping class indices (integers)\n",
    " |              to a weight (float) value, used for weighting the loss function\n",
    " |              (during training only).\n",
    " |              This can be useful to tell the model to\n",
    " |              \"pay more attention\" to samples from\n",
    " |              an under-represented class.\n",
    " |          sample_weight: Optional Numpy array of weights for\n",
    " |              the training samples, used for weighting the loss function\n",
    " |              (during training only). You can either pass a flat (1D)\n",
    " |              Numpy array with the same length as the input samples\n",
    " |              (1:1 mapping between weights and samples),\n",
    " |              or in the case of temporal data,\n",
    " |              you can pass a 2D array with shape\n",
    " |              `(samples, sequence_length)`,\n",
    " |              to apply a different weight to every timestep of every sample.\n",
    " |              In this case you should make sure to specify\n",
    " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
    " |          initial_epoch: Integer.\n",
    " |              Epoch at which to start training\n",
    " |              (useful for resuming a previous training run).\n",
    " |          steps_per_epoch: Integer or `None`.\n",
    " |              Total number of steps (batches of samples)\n",
    " |              before declaring one epoch finished and starting the\n",
    " |              next epoch. When training with input tensors such as\n",
    " |              TensorFlow data tensors, the default `None` is equal to\n",
    " |              the number of samples in your dataset divided by\n",
    " |              the batch size, or 1 if that cannot be determined.\n",
    " |          validation_steps: Only relevant if `steps_per_epoch`\n",
    " |              is specified. Total number of steps (batches of samples)\n",
    " |              to validate before stopping.\n",
    " |          **kwargs: Used for backwards compatibility.\n",
    " |      \n",
    " |      Returns:\n",
    " |          A `History` object. Its `History.history` attribute is\n",
    " |          a record of training loss values and metrics values\n",
    " |          at successive epochs, as well as validation loss values\n",
    " |          and validation metrics values (if applicable).\n",
    " |      \n",
    " |      Raises:\n",
    " |          RuntimeError: If the model was never compiled.\n",
    " |          ValueError: In case of mismatch between the provided input data\n",
    " |              and what the model expects.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function compile in module tensorflow.python.keras._impl.keras.engine.training:\n",
      "\n",
      "compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\n",
      "    Configures the model for training.\n",
      "    \n",
      "    Arguments:\n",
      "        optimizer: String (name of optimizer) or optimizer instance.\n",
      "            See [optimizers](/optimizers).\n",
      "        loss: String (name of objective function) or objective function.\n",
      "            See [losses](/losses).\n",
      "            If the model has multiple outputs, you can use a different loss\n",
      "            on each output by passing a dictionary or a list of losses.\n",
      "            The loss value that will be minimized by the model\n",
      "            will then be the sum of all individual losses.\n",
      "        metrics: List of metrics to be evaluated by the model\n",
      "            during training and testing.\n",
      "            Typically you will use `metrics=['accuracy']`.\n",
      "            To specify different metrics for different outputs of a\n",
      "            multi-output model, you could also pass a dictionary,\n",
      "            such as `metrics={'output_a': 'accuracy'}`.\n",
      "        loss_weights: Optional list or dictionary specifying scalar\n",
      "            coefficients (Python floats) to weight the loss contributions\n",
      "            of different model outputs.\n",
      "            The loss value that will be minimized by the model\n",
      "            will then be the *weighted sum* of all individual losses,\n",
      "            weighted by the `loss_weights` coefficients.\n",
      "            If a list, it is expected to have a 1:1 mapping\n",
      "            to the model's outputs. If a tensor, it is expected to map\n",
      "            output names (strings) to scalar coefficients.\n",
      "        sample_weight_mode: If you need to do timestep-wise\n",
      "            sample weighting (2D weights), set this to `\"temporal\"`.\n",
      "            `None` defaults to sample-wise weights (1D).\n",
      "            If the model has multiple outputs, you can use a different\n",
      "            `sample_weight_mode` on each output by passing a\n",
      "            dictionary or a list of modes.\n",
      "        weighted_metrics: List of metrics to be evaluated and weighted\n",
      "            by sample_weight or class_weight during training and testing.\n",
      "        target_tensors: By default, Keras will create placeholders for the\n",
      "            model's target, which will be fed with the target data during\n",
      "            training. If instead you would like to use your own\n",
      "            target tensors (in turn, Keras will not expect external\n",
      "            Numpy data for these targets at training time), you\n",
      "            can specify them via the `target_tensors` argument. It can be\n",
      "            a single tensor (for a single-output model), a list of tensors,\n",
      "            or a dict mapping output names to target tensors.\n",
      "        **kwargs: These arguments are passed to `tf.Session.run`.\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: In case of invalid arguments for\n",
      "            `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.models.Sequential.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit in module tensorflow.python.keras._impl.keras.engine.training:\n",
      "\n",
      "fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)\n",
      "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "    \n",
      "    Arguments:\n",
      "        x: Numpy array of training data (if the model has a single input),\n",
      "            or list of Numpy arrays (if the model has multiple inputs).\n",
      "            If input layers in the model are named, you can also pass a\n",
      "            dictionary mapping input names to Numpy arrays.\n",
      "            `x` can be `None` (default) if feeding from\n",
      "            TensorFlow data tensors.\n",
      "        y: Numpy array of target (label) data\n",
      "            (if the model has a single output),\n",
      "            or list of Numpy arrays (if the model has multiple outputs).\n",
      "            If output layers in the model are named, you can also pass a\n",
      "            dictionary mapping output names to Numpy arrays.\n",
      "            `y` can be `None` (default) if feeding from\n",
      "            TensorFlow data tensors.\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See [callbacks](/callbacks).\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling.\n",
      "        validation_data: tuple `(x_val, y_val)` or tuple\n",
      "            `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data.\n",
      "            `validation_data` will override `validation_split`.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch').\n",
      "            'batch' is a special option for dealing with the\n",
      "            limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      "            Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample.\n",
      "            In this case you should make sure to specify\n",
      "            `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined.\n",
      "        validation_steps: Only relevant if `steps_per_epoch`\n",
      "            is specified. Total number of steps (batches of samples)\n",
      "            to validate before stopping.\n",
      "        **kwargs: Used for backwards compatibility.\n",
      "    \n",
      "    Returns:\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    Raises:\n",
      "        RuntimeError: If the model was never compiled.\n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.models.Sequential.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function predict in module tensorflow.python.keras._impl.keras.engine.training:\n",
      "\n",
      "predict(self, x, batch_size=None, verbose=0, steps=None)\n",
      "    Generates output predictions for the input samples.\n",
      "    \n",
      "    Computation is done in batches.\n",
      "    \n",
      "    Arguments:\n",
      "        x: The input data, as a Numpy array\n",
      "            (or list of Numpy arrays if the model has multiple outputs).\n",
      "        batch_size: Integer. If unspecified, it will default to 32.\n",
      "        verbose: Verbosity mode, 0 or 1.\n",
      "        steps: Total number of steps (batches of samples)\n",
      "            before declaring the prediction round finished.\n",
      "            Ignored with the default value of `None`.\n",
      "    \n",
      "    Returns:\n",
      "        Numpy array(s) of predictions.\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: In case of mismatch between the provided\n",
      "            input data and the model's expectations,\n",
      "            or in case a stateful model receives a number of samples\n",
      "            that is not a multiple of the batch size.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.models.Sequential.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function evaluate in module tensorflow.python.keras._impl.keras.engine.training:\n",
      "\n",
      "evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n",
      "    Returns the loss value & metrics values for the model in test mode.\n",
      "    \n",
      "    Computation is done in batches.\n",
      "    \n",
      "    Arguments:\n",
      "        x: Numpy array of test data (if the model has a single input),\n",
      "            or list of Numpy arrays (if the model has multiple inputs).\n",
      "            If input layers in the model are named, you can also pass a\n",
      "            dictionary mapping input names to Numpy arrays.\n",
      "            `x` can be `None` (default) if feeding from\n",
      "            TensorFlow data tensors.\n",
      "        y: Numpy array of target (label) data\n",
      "            (if the model has a single output),\n",
      "            or list of Numpy arrays (if the model has multiple outputs).\n",
      "            If output layers in the model are named, you can also pass a\n",
      "            dictionary mapping output names to Numpy arrays.\n",
      "            `y` can be `None` (default) if feeding from\n",
      "            TensorFlow data tensors.\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per evaluation step.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "        verbose: 0 or 1. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the test samples, used for weighting the loss function.\n",
      "            You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample.\n",
      "            In this case you should make sure to specify\n",
      "            `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      "        steps: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring the evaluation round finished.\n",
      "            Ignored with the default value of `None`.\n",
      "    \n",
      "    Returns:\n",
      "        Scalar test loss (if the model has a single output and no metrics)\n",
      "        or list of scalars (if the model has multiple outputs\n",
      "        and/or metrics). The attribute `model.metrics_names` will give you\n",
      "        the display labels for the scalar outputs.\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: in case of invalid arguments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.models.Sequential.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
